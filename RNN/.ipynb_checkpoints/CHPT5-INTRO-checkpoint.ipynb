{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bd3517-073d-4529-83c8-ebb7cc3c86cf",
   "metadata": {},
   "source": [
    "# CHAPITRE 6.1 Introduction aux traitements de séquences (textes, séries temporelles)\n",
    "\n",
    "Dans ce chapitre on verra:\n",
    "\n",
    "- comment pré-traiter les données textuelles pour extraire des représentations utiles\n",
    "\n",
    "- comment utiliser les réseaux de neurones récurrents\n",
    "\n",
    "- comment utiliser les réseaux ConvNet1D pour le traitement des séquences\n",
    "\n",
    "les deux algorithmes fondamentaux dans l'apprentissage des séquences sont donc les réseaux de neurones récurrents (LSTM, GRU) et les ConvNet1D\n",
    "\n",
    "Pour celà on utiliser dans les notebooks le jeux de données IMDB pour l'analyse de sentiments et des données pour faire des prédictions météo.\n",
    "\n",
    "## I -  Traitement des données textuelles\n",
    "\n",
    "pré-traitement très important : **vectorisation** on utilise forcement des tenseurs de nombre du coup pas de texte brute:\n",
    "- segmenter en mots\n",
    "- segmenter en lettre\n",
    "- segmenter en n-grammes mots ou caractères :(plusieurs mots ou caractères\n",
    "\n",
    "**tokenisation** = le fait de décomposer un texte en token, (manière, schéma utilisé pour découper le texte en tokens)\n",
    "\n",
    "**tokens** = unité qui constituerons des variables encodé ensuite, vectorisées.\n",
    "\n",
    " une manière d'associé un vecteur à un token:\n",
    " \n",
    "**one hot encoding** et **token embedding** (**word embedding** car souvent ce sont les mots et pas les caractères qui sont utilisés)lettre\n",
    "\n",
    "**bag of word** ensemble de n-grams dont l'ordre n'importe pas. C'est une méthode de tokenisation où l'on considère les mots comme un ensemble et non une séquence.\n",
    "\n",
    "les bag of words avec des n-grams est une facon de prendre en compte les mots présents ensemble mais elle est utilisée souvent uniquement en shallow learning. Cette technique reste rigide et fragile alors que les ConvNet eux peuvent facilement prendre en compte la présence de mots les uns à coté des autres (convolution). L'utilisation de n-gramm restera donc associée à une utilisation sur du shallow learning (e.g. xgboost, régression logistique, SVM...).\n",
    "\n",
    "### I - 1 One-hot-encoding\n",
    "c'est le moyen le plus simple de transformer un token en vecteur. On associe à chaque mots un indice puis on code en présence absence sa présence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a722a518-a8e5-4ae1-8554-98445c58d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['the cat sat on the mat','the dog ate my homework']\n",
    "\n",
    "token_index = {}\n",
    "\n",
    "for sample in samples:\n",
    "    for word in sample.split(sep=' '):#split sur les espaces\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)+1\n",
    "\n",
    "print(token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da938c4-98be-468f-ba65-99fa82d533ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split(sep=' ')))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i,j,index]=1\n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938cabf-c561-421c-b9d1-9537eb6a0ab4",
   "metadata": {},
   "source": [
    "On a donc une matrice pour chaque exemple qui contient en ligne la position des mots dans l'exemple et en colone la position du mots dans le dictionnaire (le vocabulaire quoi)\n",
    "Maintenant on passe au codage one-hot en tant que telle:\n",
    "\n",
    "### one hot encoding au niveau du charactère"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd604dc1-1b1e-4246-a83a-558c2e2cc4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 101)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "samples = ['je mange une tartelette a la fraise','il fait si bon dehors']\n",
    "\n",
    "voc = string.printable\n",
    "\n",
    "dict_char = dict(zip(range(1,len(voc)+1),voc))\n",
    "\n",
    "max_length = 50\n",
    "\n",
    "results = np.zeros((len(samples), #nombre d'exemple, de phrase.. d'unité statistique\n",
    "                  max_length, #nombre de character pris dans l'exemple en ligne\n",
    "                  max(dict_char.keys())+1)) #vocabulaire en colonne\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = dict_char.get(character)\n",
    "        results[i,j,index] = 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc046f3-9680-4c7b-85b3-96d3eeaf052d",
   "metadata": {},
   "source": [
    "Un encodage à la main peut devenir assez fastidieux s'il faut praiter le texte :\n",
    "- supprimer les charactères spéciaux qui ne nous intéresse pas\n",
    "- garder les mots les plus fréquents pour éviter d'avoir des matrices creuse de trop grande dimension donc des tenseurs trop grand en entrée\n",
    "cependant suivant le contexte il faudra réaliser au moins le premier point à la main étant données que l'intérêt d'un charactère dépendra de la problématique suivit.\n",
    "\n",
    "Cependant Keras dispose d'un utilitaire qui fait le travail à notre place aussi bien pour les charactères que pour les mots\n",
    "\n",
    "### One hot encoding au niveau des séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88e037d7-0131-429f-9bcf-d742520c6c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une sequence d'indice:\n",
      " [[1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12]]\n",
      "found 12 unique token\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'je': 1,\n",
       " 'mange': 2,\n",
       " 'une': 3,\n",
       " 'tartelette': 4,\n",
       " 'a': 5,\n",
       " 'la': 6,\n",
       " 'fraise': 7,\n",
       " 'il': 8,\n",
       " 'fait': 9,\n",
       " 'si': 10,\n",
       " 'bon': 11,\n",
       " 'dehors': 12}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer # <= ici\n",
    "\n",
    "# création d'un tokenizer qui prend uniquement les 10000 mots les plus fréquents de notre corpus\n",
    "tokenizer = Tokenizer(num_words = 10000) \n",
    "\n",
    "#on le fit sur les textes\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "#transformes les chaine de chr en listes d'indices entiers\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "print('une sequence d\\'indice:\\n',sequences)\n",
    "\n",
    "#une manière de faire ça plus vite dans le cas d'une vectorisation binaireen ligne\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n",
    "\n",
    "one_hot_results\n",
    "#ici chaque ligne est un exemple et chaque colonne est l'indice d'un mot\n",
    "# ensuite codé en présence absence\n",
    "\n",
    "#pour récupérer l'indice d'un mot\n",
    "word_index = tokenizer.word_index\n",
    "print('found %s unique token' %len(word_index))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94cf1e-f8a9-497f-9e78-3086fb8479c2",
   "metadata": {},
   "source": [
    "### One hot encoding avec fonction de hashage\n",
    "l'intérêt est de ne pas avoir à créer un dictionnaire de mots en utilisant une fonction de hashage suffisamment grande pour ne pas avoir des vecteurs d'index qui se chevauchent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaf42180-df85-4e1b-a237-d59798b838e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensionality = 1000\n",
    "max_length = 50\n",
    "results = np.zeros(shape =(len(samples), max_length,dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word))%dimensionality\n",
    "        results[i,j,index]=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
