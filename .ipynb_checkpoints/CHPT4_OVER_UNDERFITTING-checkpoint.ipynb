{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improving-budapest",
   "metadata": {},
   "source": [
    "# CHPT 4 - Sur-ajustement et sous-ajustement : identification et résolution\n",
    "\n",
    "- outre l'utilisation de réseaux de neurones de capacité différentes (nombre de couche, taille des couches) deux méthodes sont couramment employées pour éviter le sur-ajustement\n",
    "- l'ajout de régularisation sur les coefficients des couches (L1 ou L2) plus souvent L2\n",
    "- l'ajout de dropout après les couches permettant d'éteindre aléatoirement des neuronesdans la couche précédentes (souvent entre 20 et 50 %)\n",
    "\n",
    "## Chargement des données et préparation des jeux de données\n",
    "Les données utilisées ici pour présenter ces différentes techniques sont issues de critique de films  du jeux de données IMDB (voir le [notebook](http://localhost:8888/lab/tree/CHPT3_DNN_IMDB.ipynb?file-browser-path=/) pour plus d'informations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data,train_labels),(test_data,test_labels) = imdb.load_data(num_words = 10000)\n",
    "\n",
    "def vectorize_data(sequences,dimension=10000):\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence]=1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_data(train_data)\n",
    "y_train = np.asarray(train_labels).astype(float)\n",
    "\n",
    "x_test = vectorize_data(test_data)\n",
    "y_test = np.asarray(test_labels).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc665aff-ced7-4b71-8741-de875ed981b4",
   "metadata": {},
   "source": [
    "## Définition de plusieurs DNN\n",
    "\n",
    "Définition de trois modèles de même capacitée (même architecture) mais avec différentes technique de traitement du sur-ajustement\n",
    "- un modèle de base avec une couche cachée\n",
    "- un modèle avec régularisation L2 sur les couches d'entrée et cachée\n",
    "- un modèle avec dropout sur les couches d'entrée et cachée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floral-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, regularizers\n",
    "\n",
    "# création du modèle originale\n",
    "def build_model():\n",
    "    \"\"\"Retourne un réseau de neurones dense à trois couches\"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation = \"relu\",input_shape = (10000,)))\n",
    "    model.add(layers.Dense(16, activation = \"relu\"))\n",
    "    model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer = \"rmsprop\",\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# création du modèle de faible capacitéeLes\n",
    "def build_model_fcap():\n",
    "    \"\"\"Retourne un réseau de neurones dense à trois couches\"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(4, activation = \"relu\",input_shape = (10000,)))\n",
    "    model.add(layers.Dense(4, activation = \"relu\"))\n",
    "    model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer = \"rmsprop\",\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# cration du modèle à forte capacitée\n",
    "def build_model_hcap():\n",
    "    \"\"\"Retourne un réseau de neurones dense à trois couches\"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation = \"relu\",input_shape = (10000,)))\n",
    "    model.add(layers.Dense(16, activation = \"relu\"))\n",
    "    model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer = \"rmsprop\",\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"])\n",
    "    return model\n",
    "# création d'un modèle avec pénalisation L2Les\n",
    "# on peut utiliser aussi : regularizers.l1() ou regularizers.l1_l2()\n",
    "def build_model_l2():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation = \"relu\",kernel_regularizer = regularizers.l2(0.001),input_shape = (10000,)))\n",
    "    model.add(layers.Dense(16, activation = \"relu\",kernel_regularizer = regularizers.l2(0.001)))\n",
    "    model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer = \"rmsprop\",\n",
    "                  loss = \"binary_crossentropy\",\n",
    "                  metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# création d'un modèle avec dropout\n",
    "def build_model_drop():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16,activation=\"relu\",input_shape = (10000,)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(16,activation=\"relu\", input_shape = (10000,)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer = \"rmsprop\",\n",
    "                 loss = \"binary_crossentropy\",\n",
    "                 metrics = [\"accuracy\"])\n",
    "    return model\n",
    "model_ori = build_model()\n",
    "model_fcap = build_model_fcap()\n",
    "model_hcap = build_model_hcap()\n",
    "model_l2 = build_model_l2()\n",
    "model_drop = build_model_drop()\n",
    "\n",
    "list_model = [model_ori,model_fcap,model_hcap,model_l2,model_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1304574c-fe52-478c-a64c-2a6d7584271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "list_loss = {np.zeros((epoch,len(list_model)))}\n",
    "for i,model in enumerate(list_model):\n",
    "    list_loss[:,i] = np.random.normal(size=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c1f46-c851-4ac8-9ddd-93f7f8944594",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model[1].fit(x_train,y_train,epochs=20,batch_size=512,metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrong-avenue",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-80784060c951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_traindata scientist évolutiondata scientist évolutiondata scientist évoldddata scientist évolutionata scientist évolutionata scientist évolutionution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
